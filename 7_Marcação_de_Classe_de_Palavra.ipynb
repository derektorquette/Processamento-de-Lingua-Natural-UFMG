{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/joXpCzdSnoW44kTpfMP6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/derektorquette/Processamento-de-Lingua-Natural-UFMG/blob/main/7_Marca%C3%A7%C3%A3o_de_Classe_de_Palavra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Mac-Morpho**\n",
        "\n",
        "baixando o corpus a fonte é o NILC"
      ],
      "metadata": {
        "id": "RsO0zv_qP4r0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmLUpR9cSBxX"
      },
      "outputs": [],
      "source": [
        "!wget http://nilc.icmc.usp.br/macmorpho/macmorpho-v3.tgz\n",
        "!tar zxvf macmorpho-v3.tgz\n",
        "\n",
        "## serão baixados três arquivos, um conjunto de treinamento, um de desenvolvimento\n",
        "## e outro arquivo de teste."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##carregando os conjuntos de treino, dev e teste."
      ],
      "metadata": {
        "id": "uASS6KL-Q9ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('macmorpho-train.txt') as f: ##comando para abrir o arquivo que baixamos\n",
        "    doc = f.read().split('\\n') ## quebra de linha\n",
        "\n",
        "doc[0].split()\n"
      ],
      "metadata": {
        "id": "v5npPxTUQ7T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mesmo código, mas faremos uma tupla com a palavra e sua classe"
      ],
      "metadata": {
        "id": "hs0O_1SkRGoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('macmorpho-train.txt') as f: ##comando para abrir o arquivo que baixamos\n",
        "    doc = f.read().split('\\n') ## quebra de linha\n",
        "\n",
        "[tuple(par.split('_')) for par in doc[0].split()]"
      ],
      "metadata": {
        "id": "H-uNYjV8QpNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('macmorpho-train.txt') as f: ##comando para abrir o arquivo que baixamos\n",
        "    doc = f.read().split('\\n') ## quebra de linha\n",
        "\n",
        "traindata = []\n",
        "for linha in doc:\n",
        "  sentenca = [tuple(par.split('_')) for par in linha.split()]\n",
        "  traindata.append(sentenca)\n",
        "\n",
        "traindata[0] ## posso procurar outras setenças mundando o valor de 0"
      ],
      "metadata": {
        "id": "QbProyG6Sotg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('macmorpho-train.txt') as f: #comando para abrir o conjunto treinamento\n",
        "    doc = f.read().split('\\n') ## quebra de linha\n",
        "\n",
        "traindata = []\n",
        "for linha in doc:\n",
        "  sentenca = [tuple(par.split('_')) for par in linha.split()]\n",
        "  traindata.append(sentenca)\n",
        "\n",
        "with open ('macmorpho-dev.txt') as f: ##comando para abrir o conjunto dev\n",
        "    doc = f.read().split('\\n') ## quebra de linha\n",
        "\n",
        "devdata = []\n",
        "for linha in doc:\n",
        "  sentenca = [tuple(par.split('_')) for par in linha.split()]\n",
        "  devdata.append(sentenca)\n",
        "\n",
        "with open ('macmorpho-test.txt') as f: ##comando para abrir o conjunto test\n",
        "    doc = f.read().split('\\n') ## quebra de linha\n",
        "\n",
        "testdata = []\n",
        "for linha in doc:\n",
        "  sentenca = [tuple(par.split('_')) for par in linha.split()]\n",
        "  testdata.append(sentenca)\n",
        "\n",
        "corpus = traindata + devdata + testdata\n"
      ],
      "metadata": {
        "id": "ng5nsphDTsgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## acessar uma sentença anotada no Mac-Morpho"
      ],
      "metadata": {
        "id": "9daw0QQ6UlPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[100]"
      ],
      "metadata": {
        "id": "KfTvx785ToEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus) #tamanho do corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6cOQpK_UzAR",
        "outputId": "db3bb3e7-d3bf-496a-961e-086cbabe6ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49934"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter ## contagem dos marcadores de palavras\n",
        "\n",
        "tagset = []\n",
        "for snt in corpus:\n",
        "  for palavra in snt:\n",
        "    tagset.append(palavra[1])\n",
        "\n",
        "Counter(tagset)\n",
        "\n"
      ],
      "metadata": {
        "id": "X2OncZ1WU8vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Corpus UD_Portuguese-GSD**\n",
        "\n",
        "baixando o corpus\n",
        "\n"
      ],
      "metadata": {
        "id": "2vGH_EOCVmYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_Portuguese-GSD.git"
      ],
      "metadata": {
        "id": "uT62fwJsV1-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## carregando os conjuntos de treino, desenvolvimento e teste do corpus"
      ],
      "metadata": {
        "id": "docOcq_jWMvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def parse(fname):\n",
        "  with open(fname) as f:\n",
        "    doc = f.read()\n",
        "  doc =  doc.split('\\n\\n')\n",
        "\n",
        "  snts = []\n",
        "  for j, inst in enumerate(doc[:-1]):\n",
        "    snt = []\n",
        "    rows = inst.split('\\n')\n",
        "    for i, elem in enumerate(rows):\n",
        "        if elem[0] != '#':\n",
        "          r = elem.split('\\t')\n",
        "          palavra, tag = r[1], r[3]\n",
        "          if tag != '_':\n",
        "            snt.append((palavra, tag))\n",
        "    snts.append(snt)\n",
        "  return snts\n",
        "\n",
        "path = 'UD_Portuguese-GSD/'\n",
        "\n",
        "traindata = parse(os.path.join(path, 'pt_gsd-ud-train.conllu'))\n",
        "devdata = parse(os.path.join(path, 'pt_gsd-ud-dev.conllu'))\n",
        "testdata = parse(os.path.join(path, 'pt_gsd-ud-test.conllu'))\n",
        "corpus = traindata + devdata + testdata #juntando num corpus só\n"
      ],
      "metadata": {
        "id": "5Ut1MAVTWSQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfOhgJCLfzOA",
        "outputId": "3042ac79-dffa-465c-d9fe-aecff1895246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12020"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "id": "UeWAppTrf80S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter ## contagem dos marcadores de palavras\n",
        "\n",
        "tagset = []\n",
        "for snt in corpus:\n",
        "  for palavra in snt:\n",
        "    tagset.append(palavra[1])\n",
        "\n",
        "Counter(tagset)\n"
      ],
      "metadata": {
        "id": "en7C7_7SgKCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Marcação de palavras baseada em regras nas UD**"
      ],
      "metadata": {
        "id": "jvq-PPDwiKxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palavra2tag = {}\n",
        "\n",
        "treino = traindata + devdata\n",
        "for snt in treino:\n",
        "  for par in snt:\n",
        "    palavra = par[0].lower()\n",
        "    classe = par[1]\n",
        "    if palavra not in palavra2tag:\n",
        "      palavra2tag[palavra] = []\n",
        "    palavra2tag[palavra].append(classe)\n",
        "\n",
        "for palavra in palavra2tag:\n",
        "  tag_count = Counter(palavra2tag[palavra])\n",
        "  most_frequent = sorted(tag_count.items(), key=lambda x: x[1], reverse=True) [0][0]\n",
        "  palavra2tag[palavra] = most_frequent\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uz0nxhaxiQE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palavra2tag['eu']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Erc2l_kdk5gx",
        "outputId": "67270e33-c5af-4d10-f0c1-0427407f11e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PRON'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  classe = palavra2tag['Thiago']\n",
        "except:\n",
        "  classe = 'NOUN'"
      ],
      "metadata": {
        "id": "WLJjkMFEikAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snt = 'pistoleiro de aluguel cobrava 500 dólar, pra mandar alguém pro beleléu, e com ele não havia xerife que parasse em pé, o xerife morria ou tinha que dar no pé'.split()\n",
        "tags = []\n",
        "\n",
        "for token in snt:\n",
        "  try:\n",
        "    classe = palavra2tag[token.lower()]\n",
        "  except:\n",
        "    classe = 'NOUN'\n",
        "  tags.append(classe)\n",
        "\n",
        "[(w, tags[i]) for i, w in enumerate(snt)] #apresenta alguns defeitos"
      ],
      "metadata": {
        "id": "Os5Ul8FllpRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Marcação de Palavras pelo Modelo Oculto de Márkov nas UD**"
      ],
      "metadata": {
        "id": "MBlRmgFQn8Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.tag import hmm\n",
        "\n",
        "tagger = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = tagger.train(traindata + devdata)"
      ],
      "metadata": {
        "id": "66dIMYfXnCsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag(\"hoje eu acordei feliz\".split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtwoLFVeo6BS",
        "outputId": "920276e0-11dc-40d9-d3aa-281fdad09268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hoje', 'ADV'), ('eu', 'PRON'), ('acordei', 'DET'), ('feliz', 'DET')]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Spacy - para métodos de Deep Learning - Redes neurais**\n",
        "\n",
        "o método já é treinado\n"
      ],
      "metadata": {
        "id": "tqRnP0Alp4C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U pip setuptools wheel\n",
        "!pip3 install -U spacy[cuda102]\n",
        "!python3 -m spacy download pt_core_news_lg"
      ],
      "metadata": {
        "id": "SjFgcYnCp13n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"pt_core_news_lg\")"
      ],
      "metadata": {
        "id": "82DNWKyCrEne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"ano passado eu morri, mas esse ano eu não morro.\")"
      ],
      "metadata": {
        "id": "LJ2tMzE_rsx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token, token.pos_)"
      ],
      "metadata": {
        "id": "_Jg5KOI8r0P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Inicializando o corpus para Avaliação**"
      ],
      "metadata": {
        "id": "C0MuAQIHtzmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def parse(fname):\n",
        "  with open(fname) as f:\n",
        "    doc = f.read()\n",
        "  doc =  doc.split('\\n\\n')\n",
        "\n",
        "  snts = []\n",
        "  for j, inst in enumerate(doc[:-1]):\n",
        "    snt = []\n",
        "    rows = inst.split('\\n')\n",
        "    for i, elem in enumerate(rows):\n",
        "        if elem[0] != '#':\n",
        "          r = elem.split('\\t')\n",
        "          palavra, tag = r[1], r[3]\n",
        "          if tag != '_':\n",
        "            snt.append((palavra, tag))\n",
        "    snts.append(snt)\n",
        "  return snts\n",
        "\n",
        "path = 'UD_Portuguese-GSD/'\n",
        "\n",
        "traindata = parse(os.path.join(path, 'pt_gsd-ud-train.conllu'))\n",
        "devdata = parse(os.path.join(path, 'pt_gsd-ud-dev.conllu'))\n",
        "testdata = parse(os.path.join(path, 'pt_gsd-ud-test.conllu'))\n",
        "corpus = traindata + devdata + testdata  #juntando num corpus só"
      ],
      "metadata": {
        "id": "_l40LwMWuzVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Método de avaliação Estado da Arte - Redes Neurais Profundas**"
      ],
      "metadata": {
        "id": "NwyVHRZNvJOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"pt_core_news_lg\")\n",
        "nlp.tokenizer = lambda x: Doc(nlp.vocab, words=x.split())\n",
        "\n",
        "y_real, y_pred = [], []\n",
        "for sentence in testdata:\n",
        "  for (token, pos) in sentence:\n",
        "    y_real.append(pos)\n",
        "\n",
        "  texto = ' '.join([w[0] for w in sentence])\n",
        "\n",
        "  doc = nlp(texto)\n",
        "  for token in doc:\n",
        "    y_pred.append(token.pos_)\n",
        "\n",
        "accscore = accuracy_score(y_real, y_pred)\n",
        "print('Acurácia Redes Neurais: ', round(accscore, 2))"
      ],
      "metadata": {
        "id": "Kalu1UbvvD95"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}